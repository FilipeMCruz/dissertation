\chapter{Implementation}
\label{chap:implementation}

This chapter addresses the implementation of the design detailed before. First, the technical decisions will be presented, followed by a technical view of the software developed. The next section explains how the software was tested by displaying some code examples. Finally, a brief synopsis closes this chapter.

\section{Technical Decisions}
\label{sec:implementation:decisions}

This section describes and justifies the decisions taken while developing \textbf{Sensae Console}.
As a green field project, \textbf{Sensae Console}, lacks constraints imposed by prior work, as such, all decisions have been taken during the thesis time span.

The following list unveils the most relevant technical decisions for \textbf{Sensae Console}:

\begin{itemize}
    \item \nameref{subsec:implementation:decisions:backend};
    \item \nameref{subsec:implementation:decisions:frontend};
    \item \nameref{subsec:implementation:decisions:graphql};
    \item \nameref{subsec:implementation:decisions:rabbitmq};
    \item \nameref{subsec:implementation:decisions:proto}
    \item \nameref{subsec:implementation:decisions:database};
    \item \nameref{subsec:implementation:decisions:drools};
    \item \nameref{subsec:implementation:decisions:js};
    \item \nameref{subsec:implementation:decisions:docker};
    \item \nameref{subsec:implementation:decisions:compose};
    \item \nameref{subsec:implementation:decisions:nginx};
    \item \nameref{subsec:implementation:decisions:git};
    \item \nameref{subsec:implementation:decisions:issues};
    \item \nameref{subsec:implementation:decisions:actions};
    \item \nameref{subsec:implementation:decisions:maven};
\end{itemize}

\subsection{Backend Technologies Usage throughout the Solution}
\label{subsec:implementation:decisions:backend}

The backend development is divided into three main areas:

\begin{itemize}
    \item \textit{iot-core} package;
    \item Data Flow Scope backend containers;
    \item Service and Configuration Scope backend containers (named \nameref{subsubsec:implementation:decisions:backend:geral});
\end{itemize}

In the following sub sections a brief description and justification of the technologies used is presented.

\subsubsection*{Programing Language Used}
\label{subsubsec:implementation:decisions:backend:prog}

As described in the development view of Section~\ref{subsubsec:design:architecture:context:development}, a package named \textit{iot-core}, an idealized \gls{SDK} for \textbf{Sensae Console}, was developed to define the information that flows inside the system. 
Since this project is still in the early stages, the \textit{iot-core} package was only developed in \textit{Java}.

In the future more programing languages may be supported though new \gls{SDK}s. The \textit{Rust} programing language is the next candidate due to its low memory footprint, fast startup times and expressive syntax.

The reasons that lead to the development of the first \gls{SDK} in \textit{Java} are:

\begin{itemize}
    \item It's the Programing language that the author is most familiarized with;
    \item Is widely used in industry for backend service development;
    \item Vast and robust support for virtually any technology used for backend development: database access, synchronous and asynchronous communication protocols, streaming platforms, embedded caches, rule engines and script engines.
\end{itemize}

The development of \textit{iot-core} in \textit{Java} lead to the development of all backend services also in \textit{Java}.

\subsubsection*{General Backend Services}
\label{subsubsec:implementation:decisions:backend:geral}

The services that this section encompasses can be seen as more robust and heavy due to their associated requirements.

As such, the framework used to develop them was \citetitle{springboot}, due to its vast documentation and big community. This framework comes with several modules that help to easily create stand-alone, production-grade applications. The author also had previously worked with this framework.

The main drawbacks of this framework are the slow start up time and high memory consumption, since these are not ideal for the microservices/cloud world.

\subsubsection*{Data Flow Scope Backend Services}
\label{subsubsec:implementation:decisions:backend:flow}

As discussed in Section~\ref{subsec:design:system_scopes:data_flow_scope}, the services that this section encompasses can be seen as more lightweight than the ones described above due to their associated requirements.

Since this containers process inbound device data, they have a bigger need to automatically scale. Since they need to react faster to throughput changes, their start up times must be small.

As such, the framework used to develop them was \citetitle{quarkus}. This framework has first-class support for \citetitle{graalvm}.

According to \cite{graalvm-intro}, GraalVM is a ``high-performance JDK designed to accelerate the execution of applications written in Java and other JVM languages while also providing runtimes for JavaScript, Python, and a number of other popular languages. GraalVM offers two ways to run Java applications: on the HotSpot JVM with Graal just-in-time (JIT) compiler or as an ahead-of-time (AOT) compiled native executable. GraalVM's polyglot capabilities make it possible to mix multiple programming languages in a single application while eliminating foreign language call costs.''

This features, coupled with the fact that the \textit{Quarkus} architecture follows the \citetitle{reactivemanifesto}, are appealing when compared with \citetitle{springboot} that only has experimental support for \citetitle{graalvm}, via \citetitle{spring-native}.

\subsection{Frontend Technologies Usage thought the Solution}
\label{subsec:implementation:decisions:frontend}

Even though a micro frontend architecture empowers the selection of different technologies depending on the requirements of the solution and team affinity with the stack, the Frontend Containers were developed using the same technological stack. At the time of writing there was only one developer involved, this diminished the cognitive load needed to work on the solution while still allowing future collaborators to use different frontend frameworks.

\subsubsection*{Programing Language and Framework Used}
\label{subsubsec:implementation:decisions:frontend:prog}

The author had previous contact with the following frameworks: (i) \citetitle{angular}, (ii) \citetitle{react}, and therefore no other tool was discussed when choosing the one to use in the solution.

The programming language used was \citetitle{typescript} since it is a strongly typed language and therefore leads to more robust and predictable code. Static typing helps to avoid various bugs that arise when using \citetitle{javascript}. Before transpiling \citetitle{typescript} code to \citetitle{javascript}, it is analyzed to detect bugs related to type errors.

As for the framework/library used, the following table, Table~\ref{tab:implementation:decisions:frontend:prog}, describes the reason that lead the author to choose Angular over React.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}cll@{}}
    \toprule
    \textbf{Framework/Library}                                                                 & \textbf{Angular} & \textbf{React}           \\ \midrule
    \begin{tabular}[c]{@{}c@{}}Separation of User Interface\\  and Business Logic\end{tabular} & enforced         & flexible                 \\ \midrule
    Language Requirements                                                                      & typescript       & javascript or typescript \\ \midrule
    Familiarity with the tool                                                                  & high             & medium                   \\ \midrule
    \begin{tabular}[c]{@{}c@{}}UI Component Libraries with wide\\  community support\end{tabular} &
      material &
      \begin{tabular}[c]{@{}l@{}}ant design, material ui, \\ react bootstrap, semantic ui react\end{tabular} \\ \bottomrule
    \end{tabular}
    \caption{Comparison of Angular with React}
    \label{tab:implementation:decisions:frontend:prog}
\end{table}

Both tools have a wide support from the community and excellent documentation. For the author, Angular outclasses React in this project since it enforces the use of good design principles via the first and second entry described in the table above. 

\subsubsection*{Technologies used to create a Micro Frontend Architecture}
\label{subsubsec:implementation:decisions:frontend:micro}

\citetitle{modulefederation} was the tool used to seemly connect the various Frontend. No other tool was considered or researched since \textit{Angular} already relies on \textit{Webpack 5} to bundle the application and therefore it's effortless to use this tool. \citetitle{modulefederation} allows programs to reference other programs parts that are not known at compile time. In addition, the micro frontends can share libraries with each other, so that the individual bundles do not contain any duplicates.

\subsection{Expose a GraphQL API On Backend Services}
\label{subsec:implementation:decisions:graphql}

The \gls{API} discussed in this section refers to the interfaces exposed to the outside world by backend containers of the Configuration and Service Scopes and isn't related to the internal communication or device data ingestion interface exposed by the Data Relayer Container.

The two approaches considered were: (i) \citetitle{rest} and (ii) \citetitle{graphql}.

According to \cite{graphql}, ``GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.''

According to \cite{rest}, ``REST APIs provide a flexible, lightweight way to integrate applications, and have emerged as the most common method for connecting components in microservices architectures.''
 
These two approaches have vast differences but they both try to answer the same question: How should one expose internal data to the outside world?

\cite{eizinger2017api}, compares these two approaches under five criteria: (i) operation reusability, (ii) discoverability, (iii) component responsibility, (iv) simplicity, (v) performance, (vi) interaction visibility and (vii) customizability.

\citetitle{graphql} was the chosen approach mainly due to better operation reusability: ``The flexibility in the definition of the exactly returned data allows clients to tailor it for their specific needs, thereby achieving highly reusable data retrieval operations.'' and interaction visibility: ``With GraphQL featuring a declarative language, intermediaries capable of understanding the GraphQL grammar can at least partly reason about the communication between a client and a GraphQL server.''

\cite{eizinger2017api}, when discussing the complexity of each approaches also highlights that ``GraphQL makes fetching data in various ways really simple for the client.''

The idea behind the highly decoupled architecture of this solution derives from the need to provide knowledgeable customers with the tools to easily design and incorporate their solutions in \textbf{Sensae Console}. The usage of \citetitle{graphql} further complements this idea by providing an API that is simple to understand and consume.

\subsection{Usage of RabbitMQ to support Internal Communication}
\label{subsec:implementation:decisions:rabbitmq}

As discussed in Section~\ref{subsec:design:alternatives:internal} and \ref{subsec:design:alternatives:flow}, the technology ultimately chosen was for internal communication was \citetitle{rabbitmq}. This message broker was chosen in detriment of others since the author had previously worked with the technology.

As discussed in the article, \citetitle{rabbitmqexpl}, the \gls{AMQP} 0.9.1 protocol defines four main concepts: (i) publisher, (ii) exchange, (iii) queue, (iv) consumer. The following diagram, Figure~\ref{fig:implementation:decisions:rabbitmq} explains how this concepts interact.

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/rabbitmq.png}
    }
    \caption[\gls{AMQP} 0.9.1 Protocol Concepts]{\gls{AMQP} 0.9.1 Protocol Concepts by \cite{rabbitmqexpl}}
    \label{fig:implementation:decisions:rabbitmq}
 \end{figure}

As discussed in \citetitle{rabbitmqexpl}, there are four types of exchanges: 

\begin{itemize}
    \item Direct Exchange: ideal for the unicast routing of messages;
    \item Fanout Exchange: ideal for the broadcast routing of messages;
    \item Topic Exchange: ideal for the multicast routing of messages, queues subscribe to specific routing keys;
    \item Header Exchange: ideal for more flexible unicast routing of messages, queues subscribe to specific message headers;
\end{itemize}

The exchange that better fits the defined requirements is the Topic Exchange.

When working with this protocol and type of exchange, some drawbacks were found:

When dealing with Topic Exchanges a Consumer can only subscribe to one specific routing key or all at once - via \textit{*}- this makes it complex to create routing keys with dynamic values. As an example, lets look at the \textit{Channel} routing key defined in Table~\ref{tab:design:domain:shared_model:routing} of Section~\ref{subsubsec:design:domain:shared_model:routing}. This key defines the single destination of a data unit. For a data unit to have various dynamic service destinations there would be a need to either:

\begin{itemize}
    \item Ensure that every single service subscribes to all relevant combinations of \textit{channel}s possible, deemed impractical;
    \item Duplicate data units, where each copy would be assigned a different channel, deemed inefficient;
\end{itemize}

To tackle this issue, another Message Broker, such as \citetitle{pulsar}, with its own protocol, can be used in the future. This Message Broker answers the drawback describe above by allowing Consumers to subscribe to multiple topics (equivalent to RabbitMQ' routing keys) on the basis of a regular expression (regex), as stated in \citetitle{pulsarmulti}.

The other drawback found is that, according to the \citetitle{rabbitmqprotocol} the routing keys have a max size of 255 bytes. As described in Table~\ref{tab:design:domain:shared_model:routing} of Section~\ref{subsubsec:design:domain:shared_model:routing}, the system currently supports various keys and more keys are expected to be added in the future, meaning that this cap may one day be reached. This limitation lead to the encoding of routing keys in a single character when possible.

\subsection{Usage of Protocol Buffers in Internal Communication}
\label{subsec:implementation:decisions:proto}

This section refers to how messages that flow in the system (via Message Broker) are serialized and deserialized. The common formats used to send structured data across systems are \gls{JSON} and \gls{XML}. This formats, when compared with \citetitle{proto} or \citetitle{thrift}, sacrifice size and de/serialization performance for human readability as stated in \cite{sumaray2012comparison}.

As mentioned before, \textbf{Sensae Console} aims to provide a good developer experience for external costumers that want to expand the solution according to their needs. Due to this, the final decision weighted heavily on formats that were self-documented, e.g. defined by a strict \textit{data schema}, such as \citetitle{proto} and \citetitle{thrift}.

This two technologies, \citetitle{proto} and \citetitle{thrift}, have similar goals and approaches to the problem they try to solve. They both rely on code generation based on a schema of the data structure. The tools related to this formats officially support various languages such as \textit{Java}, \textit{C++}, \textit{C\#}, \textit{Python}, \textit{Go} and others. 

By leveraging this features, creating a basic \gls{SDK} in a new programing language is trivial since serialization, deserialization and data structure is already taken care by the code generation tool.

\citetitle{proto} are a ``language-neutral, platform-neutral, extensible mechanism for serializing structured data''.

\citetitle{thrift}s ``primary goal is to enable efficient and reliable communication across programming languages by abstracting the portions of each language that tend to require the most customization into a common library that is implemented in each language.''

Ultimately \citetitle{proto} were chosen due to better documentation and community support.


\subsection{Database Usage throughout the Solution}
\label{subsec:implementation:decisions:database}

This section refers to how information is stored across the system.

A \gls{DBMS} is a general-purpose software system that facilitates the processes of defining, constructing, manipulating, and sharing data - \citetitle{elmasri2000fundamentals}. \gls{DBMS}s can be categorized according to several criteria, such as the data model, number of users or number of sites. This section focus on the data model, these are some of the data model types, according to \cite{elmasri2000fundamentals}: 

\begin{itemize}
    \item The \textbf{relational data model} represents a database as a collection of tables,
    where each table can be stored as a separate file; 
    \item The \textbf{document-based data model} is based on JSON (Java Script
    Object Notation) and stores the data as documents, which somewhat resemble
    complex objects; 
    \item The \textbf{column-based data model} stores the columns of rows clustered on disk pages for fast access and allow multiple versions of the data;
    \item The \textbf{graph-based data model} stores objects as graph nodes and relationships among objects as directed graph edges;
    \item The \textbf{key-value data model} associates a unique key with each value (which can be a record or object) and provides very fast access to a value given its key.
\end{itemize}

The requirements gathered unveil the need to use three different database' data models throughout the system: (i) relational, (ii) document-based and (iii) column-based data models. The following sections answer why these data models were needed and what technologies were chosen for each of them. A final section unveils an optional solution that was considered but ultimately not pursued.

\subsubsection*{Relational Database Usage}
\label{subsubsec:implementation:decisions:database:relational}

This data model has a wide variety of usage in the industry. Some of the technologies that follow this data model are: (i) \citetitle{mysql}, (ii) \citetitle{postgressql} and (iii) \citetitle{mariadb}.

It is intended for strictly structured data with well defined interrelations. This type of data can be found on most Bounded Contexts described in Section~\ref{subsec:design:domain:bounded_contexts} such as \nameref{subsubsec:design:domain:bounded_contexts:processor}, \nameref{subsubsec:design:domain:bounded_contexts:decoder}, \nameref{subsubsec:design:domain:bounded_contexts:device}, \nameref{subsubsec:design:domain:bounded_contexts:identity}, \nameref{subsubsec:design:domain:bounded_contexts:rule} and the Irrigation Zone/Device concepts of the \nameref{subsubsec:design:domain:bounded_contexts:irrigation} Context.

As such, this data model was adopted for the \textbf{Device Management Database}, \textbf{Data Decoder Database}, \textbf{Data Processor Database}, \textbf{Rule Management Database}, \textbf{Identity Management Database}, \textbf{Smart Irrigation Business Database} and \textbf{Notification Management Database} containers. 

The author had previous contact with all the cited \gls{DBMS}, the decision to use \citetitle{postgressql} was taken based on the fact that, contrary to the other options, \citetitle{postgressql} supports a vast number of Data Types such as \gls{JSON}, Arrays, \gls{UUID}, and Ranges. \citetitle{postgressql}'s data model is an extension of the relation data model, named object-relational data model - \cite{elmasri2000fundamentals}. This data model supports various concepts such as objects, classes and inheritance and therefore can lead to entity models more expressive and close to the business ideas.

\subsubsection*{Document-based Database Usage}
\label{subsubsec:implementation:decisions:database:nosql}

This data model rose from the increasing need to store and analyze unstructured data as stated by \cite{miloslavskaya2016big}.  Citing \cite{elmasri2000fundamentals}, a ``major difference between document-based systems versus object and object-relational systems (...) is that there is no requirement to specify a schema''.

This type of requirements and data resembles the Data Store context described in Section~\ref{subsec:design:system_scopes:data_flow_scope} and Figures~\ref{fig:design:architecture:container:logical:diagram:data_flow} and \ref{fig:design:architecture:container:process:diagram:flow}. This context, intended to mimic a Data Lake\footnote{Massively scalable storage repository that holds a vast amount of raw data in its native format («as is») until it is needed, by \cite{miloslavskaya2016big}}, stores any type of data for future use.

As such, this data model was adopted for the \textbf{Data Store Database} container. 

The only technology considered, and therefore adopted, was \citetitle{mongodb} due to its vast community, excellent documentation and large number of libraries that ease the database management operations. \citetitle{mongodb} also supports replication and sharding according to \cite{elmasri2000fundamentals}, this features is useful once a single node isn't capable of withstanding all data collected while providing fast access to it.

\subsubsection*{Column-based Database Usage}
\label{subsubsec:implementation:decisions:database:time}

This data model is used in applications that require large amounts of data storage, and is commonly named \textit{data warehouses}. According to \cite{dehdouh2015using}, a data warehouse  is ``designed according to a dimensional modelling which has for objective to observe facts through measures, also called indicators, according to the dimensions that represent the analysis axes''. Citing \cite{han2011survey}, this databases ``can maintain high-performance of data analysis and business intelligence processing''.

This features fit the requirements related to storing and reading vast amounts of device measures. As such, it was adopter for the \textbf{Fleet Management Database} and \textbf{Smart Irrigation Data Database} containers.

The author had no previous contact with this type of data model. Some of the technologies related to this concept are: (i) \citetitle{hbase}, (ii) \citetitle{cassandradb}, (iii) \citetitle{influxdb}, (iv) \citetitle{questdb}.

According to \cite{george2011hbase} HBase is a ``distributed, persistent, strictly consistent storage system with near-optimal write and excellent read performance''. This database uses \gls{HDFS} as its file system, and so, it is built on top of Hadoop. 
HBase does not support a structured query language like \gls{SQL}, ``even though it's comprised of a set of standard tables with rows and columns, much like a traditional database'' (\cite{ibm-hbase}).

CassandraDB is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure (\cite{lakshman2010cassandra}).
It was developed internally by Facebook and then later open-sourced to the Apache Foundation. It doesn't support \gls{SQL}.

According to \cite{naqvi2017time}, InfluxDB is an ``open-source schemaless \gls{TSDB} with optional closed-sourced components developed by InfluxData. It is written in Go programming language and it is optimized to handle time series data.'' It provides an SQL-like query language and also defines a new protocol for fast data ingestion (\cite{ilp}).

QuestDB is a relational column-oriented database designed for time series and event data and entitles it self as the ``fastest open source time series database'' (\cite{questdb}).
According to benchmarks (\cite{quest-bench}) preformed using the \gls{TSBS}, \cite{TSBS}, QuestDB ranks as the fastest option in the market.
It has out-of-the-box support for SQL Postgres wire protocol, (thus integrating with \gls{JDBC}), can be easily deployed using a single Docker Image, and also supports the \gls{ILP}.

The type of business this solution is tackling revolves around the capture and analysis of device readings, \gls{IoT}. So the notion of time has to be treated as a first class citizen. The measurements that constitute a time series are ordered on a timeline, which reveals information about underlying patterns.

As stated by \cite{naqvi2017time}, \gls{TSDB} ``can be used to efficiently store sensors and devices' data'' since, ``such technologies are generating large amount of data which is usually time-stamped''.

With this requirements in hand, a column-based data model isn't enough. The technology adopted should also natively support time series to ease data analysis. As such, the \citetitle{hbase} and \citetitle{cassandradb} options were discarded.

Between the two missing options, the author picked \citetitle{questdb} due to better support for \gls{SQL} though \gls{JDBC}. During the research of this two technologies no major downside was found for \citetitle{questdb} when compared to \citetitle{influxdb}.

\subsubsection*{Graph-based Database Usage}
\label{subsubsec:implementation:decisions:database:identity}

Even tho this data model was ultimately not used, the author deemed relevant to mention it.

As stated in the bounded context's section of~\nameref{subsubsec:design:domain:bounded_contexts:identity}, the domains follow a hierarchical structure that can resemble a graph. This context in particular would benefit from a  graph-based database, but this option was not pursued since the author had no previous contact with this family of technologies. Instead \citetitle{postgressql} was used.

\citetitle{postgressql} can represent logical hierarchical structures and concepts using the array data type as the \textit{path} from the root domain to the current domain.

Queries that revolve around graph concepts such as: select parent node, select child nodes, move nodes to a new parent and others, can be preformed efficiently using array operators such as \textbf{\&\&}, \textbf{||} and \textbf{@>}\footnote{taken from PostgresSQL Documentation: \citetitle{postgresarray} \& \citetitle{postgresarrayop}}.

\subsection{Rules Script Engine}
\label{subsec:implementation:decisions:drools}

This section refers to the bounded context of \textbf{Rule Management}. As mentioned before, the purpose of this context is to provide a high-level language that can analyze a stream of Data Units and output alerts base on them. The technology adopted was \citetitle{drools}.

\citetitle{drools} is an open-source rule engine widely used in the industry. The features that stud out from other rule engines were:

\begin{itemize}
    \item Supports for sliding windows of time;
    \item Is also a \gls{CEP} System;
    \item Integrates with the \textit{iot-core} package since it is also written in \textit{Java};
    \item Can be used as a standalone application or an embedded component of another application;
    \item Has an expressive, yet complex, syntax to write rules; 
    \item Can dynamically load rules at runtime.
\end{itemize}

The Section~\ref{subsec:implementation:description:rule} details how one can write rule scenarios.

\subsection{Data Decoders Script Engine}
\label{subsec:implementation:decisions:js}

This section refers to the bounded context of \textbf{Data Decoder}. As mentioned before, this context purpose is to translate inbound Data Units into a format and semantics that the system can understand. The technology adopted was \textit{Javascript}.

\textit{Javascript} is a high level language with an enormous community and is widely used in the industry. Another big reason behind this decision is that a lot of companies producing \gls{IoT} devices provide open-source decoders written in \textit{Javascript}, such as \href{https://github.com/Milesight-IoT/SensorDecoders}{Milesight}
\footnote{\href {https://github.com/Milesight-IoT/SensorDecoders}{github.com/Milesight-IoT/SensorDecoders}}, \href{https://github.com/SensationalSystems}{SensationalSystems}, \href{https://github.com/helium/console-decoders}{Helium}, 
\footnote{\href {https://github.com/helium/console-decoders}{github.com/helium/console-decoders}} and
\footnote{\href {https://github.com/SensationalSystems}{github.com/SensationalSystems}}. This makes it easy and straightforward to integrate new decoders in \textbf{Sensae Console}.

The Section~\ref{subsec:implementation:description:decoder} details how one can write decoders.

\subsection{Containerization of services via Docker}
\label{subsec:implementation:decisions:docker}

This section describes how the final product is packaged using \citetitle{docker}.

As stated in \citetitle{dockerinit}, Docker acts as an intermediary layer between the application to be deployed and the operating system where it will be deployed, ensuring that the developed software has the same behavior regardless of the system. The dependencies of the solution do not have to be present in the system, it is only necessary to install the Docker tool in the \gls{OS}.

This tool thus makes it possible to lower the coupling between the \gls{OS} and the software to be deployed.

With regards for this solution, each container defined in Section~\ref{subsec:design:architecture:containers} is mapped into a docker container.
A container is often compared to a virtual machine running on a hypervisor or
\gls{OS}, but it has a much lower resource consumption, since only the application is run and not not all the processes inherent to an \gls{OS}. Containers execute calls directly to the kernel running on the physical machine and can be seen, unlike virtual machines with their own kernel, as a normal process.

The system is thus represented as a collection of containers that communicate with each other and the outside through standard protocols such as HTTP or AMQP.

The production environment can thus be quickly replicated on another machine in case of a failure disaster or a overwhelming number of interaction with the server.

\subsection{Orchestration of services via Docker Compose}
\label{subsec:implementation:decisions:compose}

This section describes how the final product is orchestrated using Docker Compose.

As stated in the article \citetitle{dockercompose}, ``Compose is a tool for defining and running multi-container Docker applications''. 

Since there is no need to automatically scale the solution it was decided to use a docker compose in production inserted of tools like Kubernetes.

The solution's orchestration is defined in a \textit{YAML} file and then started with a single command. To improve security, only the needed container ports are exposed. To ensure data integrity throughout service disruptions, persistence data is mapped to folder in the \gls{OS}. To ensure an easy management of the environment, configurations are kept in the \gls{OS} and fetched by each container once they start. 

\subsection{Usage of Nginx as a web server and reverse proxy}
\label{subsec:implementation:decisions:nginx}

To serve the frontend pages and redirect requests made to backend containers, the following technologies were analyzed:

\begin{itemize}
    \item \citetitle{nginx};
    \item \citetitle{apachehttp};
    \item \citetitle{lighttpd};
\end{itemize}

All of them support the necessary requirements, but some factors lead the author to pick Nginx over the others, the following table, Table~\ref{tab:implementation:decisions:nginx:compare}, describes this criteria.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}clll@{}}
    \toprule
    \textbf{Criteria/Technology} & \textbf{Nginx} & \textbf{Apache HTTP Server} & \textbf{Lighttpd} \\ \midrule
    Resource Consumption      & low  & high      & medium \\ \midrule
    Community Size            & high & very high & medium \\ \midrule
    Familiarity with the tool & high & low       & low    \\ \bottomrule
    \end{tabular}
    \caption{Technologies Comparison - Reverse Proxy Web Server }
    \label{tab:implementation:decisions:nginx:compare}
\end{table}

\subsection{Usage of Git as a version control system of the project}
\label{subsec:implementation:decisions:git}

\citetitle{git} is a \gls{VCS}. What differentiates it from other
systems such as \citetitle{mercurial} and \citetitle{bitkeeper} is its branching model. It is currently also the most widely used.

\citetitle{github} was the platform used to host the developed code. It offers private repositories with no additional costs. This platform also has other tools such as \textit{Github Issues} and \textit{Github Actions} that ease a developer's workflow. 

A \gls{VCS} is indispensable in software development, this system allows developers to store the history of changes made to the code in an organized manner and simplifies the management of the software by the development team. This system was chosen over others because of the author was experienced with this software.

The development of the entire solution was made in two separated repositories, one for \textit{iot-core} and another for \textbf{Sensae Console}.

The \textit{iot-core} repository had a simple branching model consisting only of a master branch. 

There was an extensive use of the branching feature in the repository of \textbf{Sensae Console}, following the model shown in Figure~\ref{fig:implementation:decisions:git:branch}. The author settled for the following: a master branch that matches the deployed version, a development branch where the various features are introduced until a new version is published on the master branch, several branches dedicated to fixing bugs (hotfix) and another several branches that introduce new features and improvements (feature \textit{x}).

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/branching-model.png}
    }
    \caption[Branching Model]{Branching Model}
    \label{fig:implementation:decisions:git:branch}
\end{figure}

This model was adopted since the project was in an initial phase of development, in the future, a branching model with multiple releases, as detailed in Figure~\ref{fig:implementation:decisions:git:branch2}, is preferred. With this model one can release only the altered containers and not the entire system.

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/branching-model-2.png}
    }
    \caption[Future Branching Model]{Future Branching Model}
    \label{fig:implementation:decisions:git:branch2}
\end{figure}

This is useful when using CI/CD pipelines to compile, package and deploy the various containers of the solution. If no changes have been made to \textit{X} Container there is no need to redo all the work previously done with it.

\subsection{Usage of Github Issues to track issues, bugs and new features}
\label{subsec:implementation:decisions:issues}

As described before, the code is hosted in \citetitle{github}. One of the services that this platform offers is \textit{Github Issues}. This tool helps to track and document the development process alongside with the code.

This tool can be separated into two main views. A view is concerned about what issues, features and bugs are active in the project, Figure~\ref{fig:implementation:decisions:issues:board}, and the other is concerned with the current state of each issue, feature and bug, Figure~\ref{fig:implementation:decisions:issues:project}.

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/github-2.png}
    }
    \caption[Github Issues]{Github Issues}
    \label{fig:implementation:decisions:issues:board}
\end{figure}

Each issue has a list of tags that represent its scope and a defined milestone. With this tool, the team members can also discuss issues in depth. 

The issues presented in this page are then tracked in the \textit{project} page - Figure~\ref{fig:implementation:decisions:issues:project}. The author decided to divided the issues into 4 criteria:

\begin{itemize}
    \item \textbf{To Do}: Issues that have been discussed and are to be completed in the near future; 
    \item \textbf{In Progress}: Issues that are currently under development and have an assigned feature branches;
    \item \textbf{Done}: Issues that have been completed and have been integrated in the \textit{master} branch; 
    \item \textbf{Future}: Issues that have been purposed but have no clear deadline.
\end{itemize}

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/github.png}
    }
    \caption[Github Issues Project Board]{Github Issues Project Board}
    \label{fig:implementation:decisions:issues:project}
\end{figure}

This view helps to define a simple project roadmap and track the overall state of issues, bugs and features in the project.

\subsection{Usage of Github Actions for CI/CD}
\label{subsec:implementation:decisions:actions}

Since the code is hosted in \textit{Github}, it was decided to leverage the CI/CD features of the platform. \textit{Github Actions} purpose is to automate software workflows via CI/CD.

According to \cite{cicd}, the term CI/CD represents a method to delivering applications to clients by introducing automation into the development states.
It is divided into three concepts:

\begin{itemize}
    \item \textbf{Continuous Integration}: new versions of the project are regularly submitted, tested and merged into the current project;
    \item \textbf{Continuous Delivery}: new versions of the project are automatically archived in a repository where they can then be deployed to a production environment;
    \item \textbf{Continuous Deployment}: new versions of the project are automatically deployed to a production environment.
\end{itemize}

The \textit{iot-core} package is archived in a repository so that it can then be integrated in the backend containers of \textbf{Sensae Console}, and possibly in other projects. To do so, the team uses \textit{Github Actions}. This tool's behavior is defined in a YAML file, presented in the Code Sample~\ref{code:implementation:decisions:actions:iotcore}.

\begin{lstlisting}[style=yaml, caption=Configuration File for \textit{iot-core} Continuous Delivery, label={code:implementation:decisions:actions:iotcore}]
name: IoT Core - Continuous Delivery to maven central
on:
  push:
    tags:        
      - '**'
      - '*'
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Maven Central Repository
        uses: actions/setup-java@v1
        with:
          java-version: 17
          server-id: ossrh
          server-username: MAVEN_USERNAME
          server-password: MAVEN_PASSWORD
          gpg-private-key: ${{ secrets.MAVEN_GPG_PRIVATE_KEY }}
          gpg-passphrase: MAVEN_GPG_PASSPHRASE
      - name: Deploy with Maven
        run: mvn -B clean deploy -Pci-cd
        env:
          MAVEN_USERNAME: ${{ secrets.OSSRH_USERNAME }}
          MAVEN_PASSWORD: ${{ secrets.OSSRH_TOKEN }}
          MAVEN_GPG_PASSPHRASE: ${{ secrets.MAVEN_GPG_PASSPHRASE }}
\end{lstlisting}

As we can see in lines \textbf{2} to \textbf{6}, this action is triggered every time a new git tag is pushed to the repository.
This action then procedes to download and setup java and maven - lines \textbf{12} to \textbf{20}. Finally it runs a maven command to deploy the new version to the artifact repository - lines \textbf{21} to \textbf{26}. 

The \textbf{Sensae Console} has an action to deal with Continuous Integration - Code Sample~\ref{code:implementation:decisions:actions:cisensae}, where changes made to the software are tested.

\begin{lstlisting}[style=yaml, caption=Configuration File for \textbf{Sensae Console} Continuous Integration, label={code:implementation:decisions:actions:cisensae}]
name: Sensae Console - Continuous Integration - Test changes 
on:
  push:
  branches:
    - master
    - dev
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: "17"
          distribution: "adopt"
      - name: Test with Maven
          run: ./project/script/run-backend-tests.sh
\end{lstlisting}

As we can see in lines \textbf{2} to \textbf{6}, this action is triggered every time a new commit is push to the \textit{dev} and \textit{master} branches.
This action then procedes to download and setup java and maven - lines \textbf{10} to \textbf{16}. Finally it runs a script that tests the backend services - line \textbf{18}.

The mentioned script has the following structure - Code Sample~\ref{code:implementation:decisions:actions:testscript}.

\begin{lstlisting}[caption=Backend services test script, label={code:implementation:decisions:actions:testscript}]
#!/usr/bin/sh

ROOT_DIR=$(git rev-parse --show-toplevel)

cd "$ROOT_DIR"/project/backend-services || exit

rm --f -- ../reports/backend-test-pass.log
rm --f -- ../reports/backend-test-fail.log

ls -I data-relayer | xargs -I % sh -c 'cd % && mvn test && \
    echo % >> ../../reports/backend-test-pass.log || \
    echo % >> ../../reports/backend-test-fail.log'

test ! -f ../reports/backend-test-fail.log      
\end{lstlisting}

This script runs the command \textit{mvn test} for all backend containers and stores the results of each container in a file - lines \textbf{13} to \textbf{15}. In the end, the script checks if any container didn't pass the tests - line \textbf{15} - and exits correspondingly allowing the action to be successful or unsuccessful .

\subsection{Usage of Maven Repository to host Open-Source Code}
\label{subsec:implementation:decisions:maven}

As stated in the previous section \textit{iot-core} is delivered to an artifact repository. Since the intent of this package is to be used by any one interested on integrating his/her tool with \textbf{Sensae Console}, the artifact repository has to be publicly available. 

The Maven Central repository was the chosen one, since the \textit{maven} and \textit{gradle} tools use it, by default, to fetch dependencies.

According to the article \citetitle{centralreq} by \cite{centralreq}, to publish an artifact to maven central, a couple of additions have to be made in the \textit{pom.xml} of the project namely: (i) Supply Javadoc and Sources, (ii) Provide Files Checksums, (iii) Sign Files with GPG/PGP, (iv) Sufficient Metadata, (v) Correct Coordinates, (vi) Project Name, Description and URL, (vii) License Information, (vii) Developer Information, (viii) SCM Information.

In the \refname{***TODO***} Appendix the full \textit{pom.xml} is presented.

\section{Technical Description}
\label{sec:implementation:description}

This section guides the reader though \textbf{Sensae Console} with a technical description of the various elements that are exposed to the final costumers and platform managers.

It describes the following topics:

\begin{itemize}
    \item \nameref{subsec:implementation:description:ui};
    \item \nameref{subsec:implementation:description:api};
    \item \nameref{subsec:implementation:description:ingestion};
    \item \nameref{subsec:implementation:description:rule};
    \item \nameref{subsec:implementation:description:decoder};
    \item \nameref{subsec:implementation:description:config};
    \item \nameref{subsec:implementation:description:services};
\end{itemize}

\subsection{Description of Sensae Console UI}
\label{subsec:implementation:description:ui}

In this subsection the \gls{UI} is presented.

The Figure~\ref{fig:implementation:description:ui:home} represents the main layout for any user. It is comprised of a toolbar with a section for \textbf{Service Pages}, another for \textbf{Configuration Pages} and a final one for authentication purposes.

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/ui/home.png}
    }
    \caption[Sensae Console Home Page]{Sensae Console Home Page}
    \label{fig:implementation:description:ui:home}
\end{figure}

From this page, if the user has sufficient permissions, one can access services pages, as an example the \textbf{Smart Irrigation Page} is displayed in Figure~\ref{fig:implementation:description:ui:smartirrigation}.
This page presents a map where the user can see, search and create irrigation zones. Device measures are updated in real time via \textit{Websocket}s. The user can also see the irrigation zone details after clicking on it. From there it's possible to open/close valves an see the history of measures of each device. 

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/ui/smart-irrigation.png}
    }
    \caption[Sensae Console Smart Irrigation Page]{Sensae Console Smart Irrigation Page}
    \label{fig:implementation:description:ui:smartirrigation}
\end{figure}

From the home page the user can also access configuration pages, as an example the \textbf{Device Management Page} is displayed in Figure~\ref{fig:implementation:description:ui:device}.

\begin{figure}[H]
    \centering
    \resizebox{\columnwidth}{!}
    {
       \includegraphics{assets/figures/ui/device-management.png}
    }
    \caption[Sensae Console Device Management Page]{Sensae Console Device Management Page}
    \label{fig:implementation:description:ui:device}
\end{figure}

In this page the user can see when was the last time the device interacted with the platform, create and delete devices and edit the details of each device according to the model presented in Section\nameref{subsubsec:design:domain:bounded_contexts:device} of the \textit{Bounded Contexts}.

Other relevant pages are presented in the Appendix \textbf{***TODO***}.

\subsection{Description of Sensae Console API}
\label{subsec:implementation:description:api}

The \textbf{Sensae Console} \gls{API} is served as a \citetitle{graphql} \gls{API}, one for each service/configuration context. This \gls{API} is described with a schema.

As an example the Smart Irrigation API is presented in the Code Sample~\ref{code:implementation:description:api:irrigation}. 

\begin{lstlisting}[caption=Smart Irrigation API Schema, label={code:implementation:description:api:irrigation}]
type Subscription {
    data(filters: LiveDataFilter, Authorization: String) : SensorData
}

type Query {
    history(filters: HistoryQueryFilters) : [SensorDataHistory]
    fetchIrrigationZones : [IrrigationZone]
    fetchLatestData(filters: LatestDataQueryFilters): [SensorData]
}

type Mutation {
    createIrrigationZone(instructions: CreateIrrigationZoneCommand) : IrrigationZone
    updateIrrigationZone(instructions: UpdateIrrigationZoneCommand) : IrrigationZone
    deleteIrrigationZone(instructions: DeleteIrrigationZoneCommand) : IrrigationZone
    switchValve(instructions: ValvesToSwitch): Boolean
}        
\end{lstlisting}

From the observation of the code sample one can see that:

\begin{itemize}
    \item The \textit{data} function serves new \textit{SensorData} in realtime according to the filters provided in the \textit{filters} parameter;
    \item The textit{data} function uses \textit{Websocket} to operate as a full duplex communication channel. This spec, contrary to the HTTP spec does not account for HTTP Headers, as such the \gls{JWT} that provides the user authentication details has to be sent as a normal parameter and not as an Authorization HTTP Header.
    \item  There are three Query type functions. One to fetch the history regarding Irrigation Zones or Devices over a time span. One to fetch the Irrigation Zones. And the last one to fetch the latest data of each device;
    \item There are four mutations, each corresponding to the use cases referenced in \textbf{***TODO***}.
\end{itemize}

This functions use various types, described in Appendix \textbf{***TODO***}.

\subsection{Description of Sensae Console Data Ingestion Endpoint}
\label{subsec:implementation:description:ingestion}

The Data Ingestion Endpoint refers to how device data is sent to \textbf{Sensae Console}.

The endpoint corresponds to an HTTP POST verb with the following \gls{URL} schema:

\begin{verbatim}
    https://<ip>:<port>/sensor-data/{channel}/{infoType}/{deviceType}
\end{verbatim}

The endpoint collects the request body and then forwards it with the appropriate routing keys.

The routing keys are created according to the Table~\ref{tab:design:domain:shared_model:routing}. The \textit{infoType} can have two values: ENCODED or DECODED. Depending on this value the message is routed to \textit{Data Decoder Flow} or \textit{Data Processor Flow} as described in Figure~\ref{fig:design:architecture:container:logical:diagram:data_flow}.

The \textit{channel} parameter indicates the final service that it is destine to: \textit{fleet} for Fleet Management Service or \textit{irrigation} for Smart Irrigation Service. If another value is given the message is not routed to any service.

Finally, to ensure that the requests to this endpoint are trustworthy, a secret has to be sent in the Authorization HTTP Header. This secret is defined as a configuration of the \textbf{Sensae Console}, discussed in Section~\ref{subsec:implementation:description:config}.

\subsection{Description of Sensae Console Rule Engine}
\label{subsec:implementation:description:rule}

The rule engine can be access from the \textbf{Rule Management Page} of the UI and, as stated in \nameref{subsubsec:design:domain:bounded_contexts:rule} Bounded Context, it provides a high-level language that can be used to detect anomalies in \textbf{Data Unit}s and turn them into \textbf{Alert}s.

Valid \textbf{Data Unit}s are captured by \textbf{Alert dispatcher Backend} and the inserted in the Rule Engine.

As stated in \nameref{subsec:implementation:decisions:drools}, the rule engine used was \citetitle{drools}. To write rules for \textbf{Sensae Console} one must follow several guidelines.

A \citetitle{drools} rule is composed by conditions, actions and facts.

Facts are inserted in the rule engine. If a fact or group of facts match a condition (\textit{when} section), an action is triggered (\textit{then} section).

The rule engine, is tailored to managers or developers and not for final clients since it can be hard to create meaningfully rules without side effects.

To clarify the guidelines the following Code Samples~\ref{code:implementation:description:rule:sample1}, ~\ref{code:implementation:description:rule:sample2} and \ref{code:implementation:description:rule:sample3} are presented.

The first Code Sample presents the beginning of the rule scenario, where imports and new Facts are created.

\begin{lstlisting}[caption=Rule Scenario Example - Part 1, label={code:implementation:description:rule:sample1}]
package rules.project.two;

import pt.sharespot.iot.core.data.model.data.DataUnitReadingsDTO;
import pt.sharespot.iot.core.data.model.DataUnitDTO;
import pt.sharespot.iot.core.data.model.device.records.DeviceRecordEntryDTO;
import pt.sharespot.iot.core.data.model.properties.PropertyName;
import pt.sharespot.iot.core.alert.model.AlertBuilder;
import pt.sharespot.iot.core.alert.model.CorrelationDataBuilder;
import pt.sharespot.iot.core.alert.model.AlertLevel;
import java.util.List;
import java.util.UUID;

global pt.sharespot.iot.core.alert.model.AlertDispatcherService dispatcher;

dialect "mvel"

declare StoveSensor
    @role( event )
    deviceId : UUID
end

declare StoveSensorData
    @role( event )
    deviceId : UUID
    dataId : UUID
    temperature : Float
    humidity : Float
end
\end{lstlisting}

As we can see, from line \textbf{3} to \textbf{9}, classes from \textit{iot-core} are imported into the scenario.
At line \textbf{13} the interface that defines how an alert can be sent is imported for later use.
From line \textbf{17} to \textbf{28} two facts are declared, this can later be used as simple \textit{Java} POJOs. A fact defined with the \textit{event} role means that it occurred at a specific time (upon creation) and can be used for \gls{CEP}. 

The following code sample presents a simple rule to store \textit{StoveSensorData} facts in the working memory of \citetitle{drools}. 

\begin{lstlisting}[caption=Rule Scenario Example - Part 2, label={code:implementation:description:rule:sample2}] 
rule "Collect stove sensor data that belongs to Project #002"
    when
        $d : DataUnitDTO(
            getSensorData()
            .hasProperty(PropertyName.AIR_HUMIDITY_RELATIVE_PERCENTAGE),
            getSensorData()
            .hasProperty(PropertyName.TEMPERATURE)
        )
        exists DeviceRecordEntryDTO(
            label == "Project" && content == "#002"
        ) from $d.device.records
        not(StoveSensorData(dataId == $d.dataId))
    then
        StoveSensorData reading = new StoveSensorData();
        reading.setDeviceId($d.device.id);
        reading.setDataId($d.dataId);
        reading.setTemperature($d.getSensorData().temperature.celsius);
        reading.setHumidity($d.getSensorData().airHumidity.relativePercentage);
        insert(reading)
end
\end{lstlisting}

As we can see this rule is compossed by two sections, the \textit{when} and \textit{then} sections. In the \textit{when} the following conditions are defined:

\begin{itemize}
    \item The captured DataUnitDTO has AIR HUMIDIDTY RELATIVE PERCENTAGE and TEMPERATURE measures - lines \textbf{3} to \textbf{8};
    \item The capture DataUnitDTO has a record with a "Project" label and "\#002" content - lines \textbf{9} to \textbf{11};
    \item The DataUnitDTO is not a duplicate fact in the working memory - line \textbf{12}.
\end{itemize}

Once this conditions are meet a \textit{StoveSensorData} is created with all the needed information and then inserted into the working memory - lines \textbf{14} to \textbf{19}.

The following code sample presents a simple rule to dispatch an \textbf{Alert} after some conditions are meet. 

\begin{lstlisting}[caption=Rule Scenario Example - Part 3, label={code:implementation:description:rule:sample3}]
rule "Dispatch Stove Alarm - Dry Soil Scenario - Project #002"
    when
        $s : StoveSensorData(temperature > 26, humidity < 50)
        not(StoveSensorData(this != $s,
                temperature < 26,
                humidity > 50,
                this after[0s,11m] $s)
        )
    then 
        dispatcher.publish(AlertBuilder.create()
                            .setCategory("irrigation")
                            .setSubCategory("drySoilDetected")
                            .setDescription("Project #002 - Device "+ $s.deviceId +" detected low humidity/high temperature")
                            .setLevel(AlertLevel.ADVISORY)
                            .setContext(CorrelationDataBuilder.create()
                                .setDeviceIds($s.deviceId)
                                .setOther("Project #002")
                                .build())
                            .build());
end
\end{lstlisting}

As we can see this rule matches when the same device reports measures of air humidity higher than 50\% and temperature lower then 26 ºC for more than 11 minutes.

Once it matches an Alert is dispatched using the referenced dispatcher in Code Sample~\ref{code:implementation:description:rule:sample1}. The Alert can be created using the builder pattern.

An Alert closely resembles a Notification from the \nameref{subsubsec:design:domain:bounded_contexts:notification} Bounded Context. It also has a category (line \textbf{13}), a sub category (line \textbf{14}), a severity level (line \textbf{16}), a description (line \textbf{15}) and a notification context (lines \textbf{17} to \textbf{20}).

For an \textbf{Alert} to be sent at least the category and sub category parameters have to be set. By default the \textbf{INFORMATION} severity level is used.

In order for services to act upon a received \textbf{Alert}, it has to be associated with a \textit{DeviceId} (this association helps services like \textbf{Smart Irrigation} to know what Valve must be turned on or off), a \textit{DataId} or \textit{Other}.

An \textbf{Alert} is later transformed and store as a Notification, the \textit{DeviceId}s associated to it are used to determine what domains will have access to the Notification. If no \textit{DeviceId}s are associated only the root domain will have access to it.

\subsection{Description of Sensae Console Data Decoders}
\label{subsec:implementation:description:decoder}

As mentioned in the \nameref{subsubsec:design:domain:bounded_contexts:decoder} Bounded Context Section, \textbf{Data decoder}'s purpose is to provide a flexible option to transform inbound data units into something that the system understands.

This happens when a \textbf{Data Unit} has a routing key with the ENCODED info type.

There are certain guidelines to follow in order to create a decoder:

\begin{itemize}
    \item Has to be written in vanilla \textit{javascript};
    \item Has to have an \textit{entry} function with the following signature \textit{function convert(dataUnit)};
    \item Can't import any node function, npm package or reference other scripts.
\end{itemize}

As an example, the Code Sample~\ref{code:implementation:description:decoder:em300th} presents the decoder for the device type EM500-TH\footnote{https://www.milesight-iot.com/lorawan/sensor/em300-th}.

\begin{lstlisting}[caption=EM300-TH Data Decoder Example, label={code:implementation:description:decoder:em300th}] 
const decodePayload = (payload, port) =>
    ({"0": decoder(base64ToHex(payload), port)});

const base64ToHex = (() => {
    const values = [],
    output = [];

    return function base64ToHex(txt) {
        if (output.length <= 0) populateLookups();
        const result = [];
        let v1, v2, v3, v4;
        for (let i = 0, len = txt.length; i < len; i += 4) {
            v1 = values[txt.charCodeAt(i)];
            v2 = values[txt.charCodeAt(i + 1)];
            v3 = values[txt.charCodeAt(i + 2)];
            v4 = values[txt.charCodeAt(i + 3)];
            result.push(
                parseInt(output[(v1 << 2) | (v2 >> 4)], 16),
                parseInt(output[((v2 & 15) << 4) | (v3 >> 2)], 16),
                parseInt(output[((v3 & 3) << 6) | v4], 16)
            );
        }
        if (v4 === 64) result.splice(v3 === 64 ? -2 : -1);
        return result;
    };
    function populateLookups() {
        const keys =
            "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=";
        for (let i = 0; i < 256; i++) {
            output.push(("0" + i.toString(16)).slice(-2));
            values.push(0);
        }
        for (let i = 0; i < 65; i++) values[keys.charCodeAt(i)] = i;
    }
})();

function decoder(bytes, port) {
    let decoded = {}, temperature = {}, airHumidity = {}, battery = {};
    for (let i = 0; i < bytes.length;) {
        let channel_id = bytes[i++];
        let channel_type = bytes[i++];
        if (channel_id === 0x01 && channel_type === 0x75) {
            decoded.battery = battery;
            battery.percentage = bytes[i];
            i += 1;
        } else if (channel_id === 0x03 && channel_type === 0x67) {
            decoded.temperature = temperature;
            temperature.celsius = readInt16LE(bytes.slice(i,i+2))/10;
            i += 2;
        } else if (channel_id === 0x04 && channel_type === 0x68) {
            decoded.airHumidity = airHumidity;
            airHumidity.relativePercentage = bytes[i] / 2;
            i += 1;
        } else {
            break;
        }
    }
    return decoded;
}
const readUInt16LE = bytes => (bytes[1] << 8) + bytes[0] & 0xffff;

function readInt16LE(bytes) {
    let ref = readUInt16LE(bytes);
    return ref > 0x7fff ? ref - 0x10000 : ref;
}
const convert = dataUnit => ({
    dataId: dataUnit.uuid,
    reportedAt: dataUnit.reported_at,
    device: {
        id: dataUnit.id,
        name: dataUnit.name,
        downlink: dataUnit.downlink_url,
    },
    measures: decodePayload(dataUnit.payload, dataUnit.port),
});
\end{lstlisting}

As we can see, this code sample decodes an EM300-TH \textbf{Data Unit}. The function \textit{convert} is the one mentioned in the guidelines, it assigns values such as \textit{id}, \textit{name}, \textit{reported\_at}, \textit{downlink\_url}, \textit{uuid} to its correct place and calls the function \textit{decodePayload} to gather the device measures. The \textit{decodePayload} stores every measure in the \textit{controller} key - value \textit{0}. The function \textit{base64ToHex} is the function that reads a Base 64 string and transforms it into a Hex Array - to reduce bandwidth th device normally encodes and sends data as a base 64 string. The function \textit{decoder}, \textit{readInt16LE} and \textit{readUInt16LE} were adapted from the TTN decoder\footnote{https://github.com/Milesight-IoT/SensorDecoders/blob/master/EM300\_Series/EM300-TH} of this device.

\subsection{Description of Configuration Files}
\label{subsec:implementation:description:config}

This section describes how a \textbf{Sensae Console} is configured. One of the problems that arise from a microservice architecture is how to maintain all configurations for each container developed and configured. following the \citetitle{confcentral}, all configurations are defined via configuration files that support three environments: \textit{dev}, \textit{test} and \textit{prod}.

This configurations are defined, for each environment, in a single file. This file, Listing~\ref{code:implementation:description:config:file}, has the following structure:

\begin{lstlisting}[caption=Configuration File for Production Environment, label={code:implementation:description:config:file}]
export SENSAE_MAPBOX_ACCESS_TOKEN=
export SENSAE_MAPBOX_SIMPLE_STYLE=
export SENSAE_MAPBOX_SATELLITE_STYLE=
export SENSAE_BROKER_USERNAME=
export SENSAE_BROKER_PASSWORD=
export SENSAE_COMMON_DATABASE_PASSWORD=
export SENSAE_DATA_STORE_USER_PASSWORD=
export SENSAE_DATA_STORE_ROOT_PASSWORD=
export SENSAE_AUTH_PATH_PUB_KEY=
export SENSAE_AUTH_PATH_PRIV_KEY=
export SENSAE_AUTH_ISSUER=
export SENSAE_AUTH_AUDIENCE=
export SENSAE_DATA_AUTH_KEY=
export SENSAE_AUTH_EXTERNAL_MICROSOFT_AUDIENCE=
export SENSAE_AUTH_EXTERNAL_GOOGLE_AUDIENCE=
export SENSAE_SMS_TWILIO_ACCOUNT_SID=
export SENSAE_SMS_TWILIO_AUTH_TOKEN=
export SENSAE_SMS_SENDER_NUMBER=
export SENSAE_SMS_ACTIVATE=
export SENSAE_EMAIL_SENDER_ACCOUNT=
export SENSAE_EMAIL_SUBJECT=
export SENSAE_EMAIL_SENDER_PASSWORD=
export SENSAE_EMAIL_SMTP_HOST=
export SENSAE_EMAIL_SMTP_PORT=
export SENSAE_EMAIL_ACTIVATE=
export SENSAE_PROD_PUBLIC_DOMAIN=
export SENSAE_ADMIN_EMAIL=
\end{lstlisting}

This file variables are then passed on to each container's environment configuration file with the help of a script. The Code Sample~\ref{code:implementation:description:config:script} sheds a light on how the script propagates the configurations.

\begin{lstlisting}[caption=Configuration Propagation Script, label={code:implementation:description:config:script}]
#!/usr/bin/sh

ROOT_DIR=$(git rev-parse --show-toplevel)

cd "$ROOT_DIR"/project || exit

. ./secrets/prod.conf

SECRET_BACK=secrets/templates/prod/backend-services
SECRET_FRONT=secrets/templates/prod/frontend-services
SECRET_DB=secrets/templates/prod/databases

BACK_PREFFIX=secrets/prod
FRONT_PREFFIX=frontend-services/apps
FRONT_SUFFIX=src/environments/environment.prod.ts

envsubst < $SECRET_BACK/alert-dispatcher-backend.env > \
     $BACK_PREFFIX/alert-dispatcher-backend.env
# and all other backend services
envsubst < $SECRET_BACK/data-validator.env >
     $BACK_PREFFIX/data-validator.env

envsubst < $SECRET_FRONT/device-management-frontend.ts > \
 $FRONT_PREFFIX/device-management-frontend/$FRONT_SUFFIX
# and all other frontend services
envsubst < $SECRET_FRONT/ui-aggregator.ts > \
 $FRONT_PREFFIX/ui-aggregator/$FRONT_SUFFIX

envsubst < secrets/templates/prod/message-broker/message-broker.env > \
 $BACK_PREFFIX/message-broker.env

envsubst < $SECRET_DB/data-decoder-database.env > \
 $BACK_PREFFIX/data-decoder-database.env
# and all other databases
envsubst < $SECRET_DB/rule-management-database.env > \
 $BACK_PREFFIX/rule-management-database.env
\end{lstlisting}

In the future, as more isolated deployments are made, a tool such as \citetitle{vault} should be integrated in the solution.

\subsection{Description of Sensae Console Services}
\label{subsec:implementation:description:services}

This section refers how services interact with the \textbf{Data Flow Scope}, this was briefly mentioned in Section~\ref{subsubsec:design:domain:shared_model:routing}.

In order to provide an easy to understand integration with the \textbf{Data Flow Scope}, the routing keys concept was introduced. The idea, from the point of view of someone developing a service, is to start by defining what type of information that service should capture.

The two types of information a service can capture are: (i) \textbf{Data Unit}s and (ii) \textbf{Alert}s. Each of this information are defined by their routing keys as described in Table~\ref{tab:design:domain:shared_model:routing}.

A service can also publish \textbf{Command}s.

The following sub sections will detail each service information needs.

\subsubsection*{Smart Irrigation Service}
\label{subsubsec:implementation:description:services:irrigation}

This service captures information of the given types:

\begin{itemize}
    \item \textbf{Data Topic}: \textit{'processed'}, \textit{'correct'}, with \textit{'defined ownership'} and \textit{'device information'} data unit with \textit{'gps'} and \textit{'trigger'} readings in the channel \textit{'irrigation'} (for valves);
    \item \textbf{Data Topic}: \textit{'processed'}, \textit{'correct'}, with \textit{'defined ownership'} and \textit{'device information'} data unit with \textit{'gps'}, \textit{'temperature'} and \textit{'air humidity'} readings in the channel \textit{'irrigation'} (for green house sensors);
    \item \textbf{Data Topic}: \textit{'processed'}, \textit{'correct'}, with \textit{'defined ownership'} and \textit{'device information'} data unit with \textit{'gps'}, \textit{'illuminance'} and \textit{'soil moisture'} readings in the channel \textit{'irrigation'} (for park sensors);
    \item \textbf{Alert Topic}: alerts with the category \textit{'smartIrrigation'} and sub category \textit{'drySoil'} (to open all valves in a garden);
    \item \textbf{Alert Topic}: alerts with \textit{'defined ownership'}, the category \textit{'smartIrrigation'} and sub category \textit{'moistSoil'} (to close all valves in a garden);
    \item \textbf{Alert Topic}: alerts with \textit{'defined ownership'}, the category \textit{'smartIrrigation'} and sub category \textit{'valveOpenForLengthyPeriod'} (to close that specific valve).
\end{itemize}

It then publishes Commands to close or open valves. The service can only issue a command if the Data Unit sent by the valve refers two commands, one to open and another to close the valve. This commands, usually defined in the \textbf{Device Management Page}, and mentioned in the \nameref{subsubsec:design:domain:bounded_contexts:device} Bounded Context, need to have the \textit{CommandId} value as \textit{'openValve'} or \textit{'closeValve'}.

At a high-level view, this service requires data from \textit{Park} sensors, \textit{Green Houses} and \textit{Valves} that flow in the \textit{'irrigation'} channel. It captures Alerts to decide when to open or close Valves by sending specific Commands.

\subsubsection*{Fleet Management Service}
\label{subsubsec:implementation:description:services:fleet}

This service captures information of a single type (and doesn't publish any Command):

\textbf{Data Topic}: \textit{'processed'}, \textit{'correct'}, with \textit{'defined ownership'} and \textit{'device information'} data unit with \textit{'gps'} readings in the channel \textit{'fleet'}.

At a high-level view, this service only requires \gls{GPS} data sent to the \textit{'fleet'} channel.

\subsubsection*{Notification Management Service}
\label{subsubsec:implementation:description:services:notification}

This service captures information of a single type (and doesn't publish any Command):

\textbf{Alert Topic}: alerts with \textit{'defined ownership'}.

At a high-level view, this service requires all alerts that already have a \textit{'defined ownership'}.

\section{Testing}
\label{sec:implementation:testing}

\subsection{Unit Tests}
\label{subsec:implementation:tests:unit}

\subsection{Integration Tests}
\label{subsec:implementation:tests:integration}

\subsection{Functional Tests}
\label{subsec:implementation:tests:functional}

\subsection{End-to-End Tests}
\label{subsec:implementation:tests:endtoend}

\subsection{Architectural Tests}
\label{subsec:implementation:tests:arch}

\subsection{Performance Tests}
\label{subsec:implementation:tests:performance}

\section{Synopsis}
\label{sec:implementation:synopsis}
